---
title: "Product Segmentation"
author: "Seung Hyun Sung"
date: "2/4/2022"
output: 
    html_document:
        theme: flatly
        toc: true
        toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE)
```

```{r libraries}
# libraries
library(knitr)
library(timetk)
library(tidytext)
library(plotly)
library(ggwordcloud)
library(ggiraphExtra)
library(fs)
library(readr)
library(tidyverse)
library(tidymodels)
library(tidyquant)
library(recipes)
library(umap)
# Python integration 
library(reticulate)
```

## Setup R' spython interface (conda py3.8 Environment)

conda create -n py3.8 python=3.8 scikit-learn pandas numpy matplotlib
 This code does the following:
   - Creates a new Python environment called “py3.8”
   - Installs python version 3.8
   - Installs the latest versions of scikit-learn, pandas, numpy, and matplotlib.

Replace this with your conda environment containing sklearn, pandas, & numpy


# 1. Data 

```{r}
retail_order_tbl <-  read_rds("00_Data/data_wranggled/trans_data/retail_order_tbl.rds")

products_manual_tbl <-  read_rds("00_Data/data_wranggled/product_data/products_manual_tbl.rds")

time_split_train <- read_rds("00_Data/data_split/time_split_train.rds")
```


```{r}
train_X_tbl  <- time_split_train$train
train_Y_tbl  <- time_split_train$test
```


```{r}
analyse_tbl <- train_X_tbl
```


# 2. Plan Customer-Product (Unsupervised) Clustering 

__Problems:__
- Products are highly unique and has no categories 
- Need categories to learn which customers are similar in *purchasing behaviour* 

__Solution:__
- Price Feature Engineering will help group products by price categories 
-  Text Feature Engineering is essential to compare description fields
- Stock Code Engineering will group products with similar catalogue

__Hypothesis:__
- Similar Products Embeds Similar Word Token 
- Similar Products Have Similar Price Variations 
- Similar Products Have Similar Stock Codes (digits)

__Algorithm:__
- Kmeans: Distance-Based Metric Learning 
- DBSCAN:  Density-Based Metric Learning 

__Evaluation:__
- Summarise cluster result of its key components + comparative analysis 
- Perform double-clustering the result with customer purchase behaviour statistics (RFM metric) to evaluate these product category roles. 




# 3. Products Feature Engineering (Extrapolation) 

## 3.1 Text Feature Engineering

### Term Extraction - Unnesting Tokens

-- Use 'tidytext' to unnest tokens 
-- Stems the tokens using 'hunspell' to return only the root of the word 

```{r}
products_dictionary_tbl <- products_manual_tbl %>% 
    select(stock_code, mode_description) %>% 
    unnest_tokens(terms, mode_description, token = "words") %>% 
    filter(!(nchar(terms) == 1)) %>% 
    mutate(terms = hunspell::hunspell_stem(terms)) %>% 
    unnest(terms) %>% 
    mutate(n = 1)

```


### Term Frequency

-- Count Frequency 
-- Remove Stop words (e.g. "of" has no meaning)
-- Remove colour terms(e.g. "pink" has no meanings)
-- Remove terms with numbers (e.g. "130.5cm" has no meaning)

```{r}
terms_frequency_tbl <- products_dictionary_tbl %>% 
    # Remove unnecessary terms 
    anti_join(stop_words, by = c("terms" = "word")) %>% 
    # Remove colour terms 
    filter(!terms %in% colours()) %>% 
    # Remove terms with numbers 
    filter(!terms %>% str_detect(pattern = "[0-9]")) %>% 
    # summarise -0> abstract frequency 
    group_by(terms) %>% 
    summarise(
        n = sum(n)
    ) %>% 
    arrange(desc(n))

```


### Term Frequency Visualisation

__Plan:__

- Simplify the algorithm by filtering top frequent terms  

- Eliminate uninformative descriptive terminology:

  -- Cumulative Sum (frequency threshold)
  
  -- Near Zero Variance (variance threshold) 

```{r}
source("00_Functions/Visualisation_function/visual.R")
```


Cumulative Plot 

-- Flexible Filtering: Rank & Filter terms by Cumulative Sum

-- Cumulative Sum above 50% -> Relevant Term

-- Basis on the descending gradient of cumulative sum the threshold is selected 

```{r}
rank_term_frequency_tbl <- terms_frequency_tbl %>% 
  get_rank_cumulative(.cumulative_max = 0.5)

rank_term_frequency_tbl %>% plot_rank_cumulative(max_slice = 500)

```

Word Cloud Plot 

```{r}
terms_frequency_tbl %>% 
    slice(1:100) %>% 
    mutate(terms = as_factor(terms) %>% fct_rev()) %>% 
    ggplot() + 
    geom_text_wordcloud_area(aes(label = terms, size = n, colour = n)) +
    scale_size_area(max_size = 14) +
    scale_colour_viridis_c(direction = -1) +
    theme(plot.background = element_rect(fill = "black"),
          panel.background = element_rect(fill = "black"))
```


Filter Top 100 Term  

```{r}
top_100_terms <- terms_frequency_tbl %>% slice(1:100) %>% pull(terms)

product_term_feature_tbl <- products_dictionary_tbl %>% 
    filter(terms %in% top_100_terms) %>% 
    pivot_wider(
        names_from = terms,
        values_from = n,
        values_fill = list(n = 0),
        values_fn = list(n = sum)
    )
```


## 3.2 Price Feature Engineering 

__Problems:__
- Price Variations
- Highly Skewed (Critical damage for distance based algorithm) 

__Solution:__
- Implement Simple Statistics of product's Price Range 
- Flexible Transformation (Data Pre-processing) -> Box-Cox or Yeo-Johnson

```{r}
product_price_range_tbl <- analyse_tbl %>% 
  group_by(stock_code) %>% 
  summarise(
    .groups     = "drop",
    n_prices    = n(),
    med_price   = median(unit_price) %>% round(2),
    mean_price  = median(unit_price) %>% round(2),
    min_price   = min(unit_price),
    p25_price   = quantile(unit_price)[2],
    p50_price   = quantile(unit_price)[3],
    p75_price   = quantile(unit_price)[4],
    max_price   = max(unit_price),
    range_price = ((max_price - min_price)/ mean_price) %>% round(2)
  ) 
```


## 3.3 Stock Code Feature Engineering 

A stock code here will not be *stock keeping unit*. Depending on the type of inventory, retails include identifying information for everything from department to style, target customers, rountine, seasonal/occasional etc to identifies a product and helps business to track inventory for retail business. 

```{r}
product_code_tbl <- product_term_feature_tbl %>% 
  select(stock_code) %>% 
  tidyr::separate(stock_code, 
           into = c("digit_code", "string_code"), 
           sep = "(?<=[A-Za-z])(?=[0-9])|(?<=[0-9])(?=[A-Za-z])",
           remove = FALSE
           ) %>% 
  mutate(code = str_replace(digit_code, "[a-zA-Z ]", ""))  %>% 
  separate(col = code,
           into = str_c("code_digit_", 6:1),
           sep = "",
           remove = TRUE) %>% 
  select(-code_digit_6, -digit_code) %>% 
  mutate(string_code = replace_na(string_code, 0)) %>% 
  mutate_at(vars(!starts_with("stock_")), funs(as.factor)) 
```


# 4 Feature Preprocessing 

- Pre-processing steps

Step_1: Remove near zero variance feature.This step will remove most of the tokenised term features with its variance below given frequency threshold 

Step_2: YeoJohnson transformation will be operated to numeric price data with heavily skewness

Step_3 & 4: kmeans & DBSCAN algorithm relies on the distance metrics (e.g. euclidean distance, density) hence it is important to center and scale the data


```{r}
product_joined_tbl <- product_term_feature_tbl %>% 
  left_join(product_code_tbl) %>% 
  left_join(product_price_range_tbl) %>% 
  # Observed that some products are missing from the training data 
  # New Product!! 
  filter(complete.cases(.))

# Pre-processing steps for distance-based algorithms 
recipe_obj <- recipe(~., product_joined_tbl) %>% 
  step_rm(stock_code) %>% 
  step_dummy(all_nominal()) %>% 
  step_nzv(all_predictors()) %>% 
  step_BoxCox(contains("_price")) %>% 
  step_range(contains("_price")) %>% 
  prep() 

prep_product_cluster_tbl <- recipe_obj %>% bake(new_data = product_joined_tbl)

X_train <- prep_product_cluster_tbl 
Y_label <- product_joined_tbl %>% select(stock_code) 
```


# 5 Clustering  

-- Kmeans Clustering 
-- Scree Plot: To find the optimal k number of clusters
-- 2-D UMAP: For visualisation puropose  


## 5.1 SetUp

```{r}
source("00_Functions/Visualisation_function/optimal_kmeans_skree_plot.R")
source("00_Functions/Iterative_analysis/product_disctionary.R")
```


```{r}
kmeans_mapper <- function(center = 3) {
    set.seed(42)
    X_train %>%
        kmeans(centers = center, nstart = 20)
}

centers_tbl <- tibble(centers = 1:20)

k_means_mapped_tbl <- centers_tbl %>% 
    mutate(k_means = centers %>% map(kmeans_mapper),
           glance = k_means %>% map(broom::glance))
```


## 5.2 Parameter Selection: Number of K-clustering Selection 
-- k = 8 centers 

```{r}
k_means_mapped_tbl %>% plot_kmeans_scree(.metric = tot.withinss)
```


## 5.3 Result Visualisation 

-- UMAP (Dimensional Reduction)

```{r}
umap_results <- X_train %>% 
    umap()

umap_results_tbl <- umap_results$layout %>% 
    as_tibble() %>% 
    setNames(c("x", "y")) %>% 
    cbind(X_train)
```

-- Mrage (UMAP + Kmeans) Result 

-- Save Result Table 

```{r}
k_means_obj <- k_means_mapped_tbl %>% 
  # Indicate the optimal K selected
  filter(centers == 8) %>% 
  pull(k_means) %>%  pluck(1)

umap_kmeans_results_tbl <- k_means_obj %>% broom::augment(X_train) %>% bind_cols(Y_label) %>% 
    select(stock_code, .cluster) %>% 
    bind_cols(umap_results_tbl)

umap_kmeans_results_tbl %>% 
    ggplot(aes(x, y, colour = .cluster)) +
    geom_point(alpha = 0.5) +
    theme_tq() +
    scale_colour_tq()
```


```{r}
code_bd_kmeans_10_result_tbl <- umap_kmeans_results_tbl %>%
  left_join(
    products_manual_tbl %>% select(-median_unit_price, -n)
    )
```


### Finalise 

```{r}
product_cluster_tbl <- code_bd_kmeans_10_result_tbl
```



# 6 Product Category Evaluation Via Visualisation 
-- Summarise Clustering Components by Radar Visualisation 
-- Evaluate Stemming Words of the Clustered Product Groups Using Heat Map 


```{r}
term_freq_by_cluster_tbl <- product_cluster_tbl %>%
  product_disctionary() %>% 
  # Group by cluster & select top N terms
  group_by(.cluster, terms) %>% 
  summarise(n = sum(n)) %>% 
  arrange(desc(n), by_group = TRUE) %>% 
  slice(1:50)
```

```{r}
g <- term_freq_by_cluster_tbl %>% 
  slice(1:10) %>% 
  ungroup() %>% 
  arrange(desc(n)) %>%
  mutate(terms = terms %>% as_factor %>% fct_reorder(n)) %>% 
  ggplot(aes(n, terms, colour = .cluster)) +
  geom_point() +
  expand_limits(x = 0) +
  facet_wrap(~.cluster, ncol = 2, scales = "free_y") +
  theme_tq() +
  scale_colour_tq() +
  labs(y = "") +
  theme(legend.position = "none")
  
ggplotly(g)
```



# Save 
-- kmeans k = 10 (optimal observed by tot.withinss) result table
-- Final Product Cluster Table 

```{r}
write_rds(code_bd_kmeans_10_result_tbl, "00_Data/data_engineered/code_bd_kmeans_10_result_tbl.rds")

dump("product_disctionary", "00_Functions/Iterative_analysis/product_disctionary.R")
```







